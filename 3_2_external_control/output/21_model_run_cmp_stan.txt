Inference for Stan model: anon_model.
4 chains, each with iter=1e+05; warmup=10000; thin=10; 
post-warmup draws per chain=9000, total post-warmup draws=36000.

                mean se_mean     sd     2.5%      25%      50%      75%    97.5% n_eff Rhat
beta_ext       -3.69    0.01   0.20    -4.11    -3.82    -3.69    -3.55    -3.32   384 1.01
tau           476.35  109.65 392.55    20.54   174.51   380.99   653.96  1479.19    13 1.11
beta_trt       -0.45    0.01   0.13    -0.70    -0.54    -0.45    -0.36    -0.20   314 1.01
beta0          -3.69    0.01   0.20    -4.11    -3.82    -3.69    -3.55    -3.32   382 1.01
r               1.07    0.00   0.05     0.97     1.04     1.07     1.11     1.18   424 1.01
lambda[1]       0.02    0.00   0.00     0.01     0.01     0.02     0.02     0.02   562 1.00
lambda[2]       0.03    0.00   0.01     0.02     0.02     0.02     0.03     0.04   387 1.01
lambda[3]       0.02    0.00   0.00     0.01     0.01     0.02     0.02     0.02   562 1.00
lambda[4]       0.02    0.00   0.00     0.01     0.01     0.02     0.02     0.02   562 1.00
lambda[5]       0.03    0.00   0.01     0.02     0.02     0.02     0.03     0.04   387 1.01
lambda[6]       0.02    0.00   0.00     0.01     0.01     0.02     0.02     0.02   562 1.00
lambda[7]       0.03    0.00   0.01     0.02     0.02     0.02     0.03     0.04   387 1.01
lambda[8]       0.03    0.00   0.01     0.02     0.02     0.02     0.03     0.04   387 1.01
lambda[9]       0.02    0.00   0.00     0.01     0.01     0.02     0.02     0.02   562 1.00
lambda[10]      0.03    0.00   0.01     0.02     0.02     0.02     0.03     0.04   387 1.01
lambda[11]      0.02    0.00   0.00     0.01     0.01     0.02     0.02     0.02   562 1.00
lambda[12]      0.02    0.00   0.00     0.01     0.01     0.02     0.02     0.02   562 1.00
lambda[13]      0.02    0.00   0.00     0.01     0.01     0.02     0.02     0.02   562 1.00
lambda[14]      0.02    0.00   0.00     0.01     0.01     0.02     0.02     0.02   562 1.00
lambda[15]      0.03    0.00   0.01     0.02     0.02     0.02     0.03     0.04   387 1.01
lambda[16]      0.02    0.00   0.00     0.01     0.01     0.02     0.02     0.02   562 1.00
lambda[17]      0.03    0.00   0.01     0.02     0.02     0.02     0.03     0.04   387 1.01
lambda[18]      0.02    0.00   0.00     0.01     0.01     0.02     0.02     0.02   562 1.00
lambda[19]      0.03    0.00   0.01     0.02     0.02     0.02     0.03     0.04   387 1.01
lambda[20]      0.03    0.00   0.01     0.02     0.02     0.02     0.03     0.04   387 1.01
lambda[21]      0.03    0.00   0.01     0.02     0.02     0.02     0.03     0.04   387 1.01
lambda[22]      0.03    0.00   0.01     0.02     0.02     0.02     0.03     0.04   387 1.01
lambda[23]      0.02    0.00   0.00     0.01     0.01     0.02     0.02     0.02   562 1.00
lambda[24]      0.02    0.00   0.00     0.01     0.01     0.02     0.02     0.02   562 1.00
lambda[25]      0.02    0.00   0.00     0.01     0.01     0.02     0.02     0.02   562 1.00
lambda[26]      0.03    0.00   0.01     0.02     0.02     0.02     0.03     0.04   387 1.01
lambda[27]      0.02    0.00   0.00     0.01     0.01     0.02     0.02     0.02   562 1.00
lambda[28]      0.02    0.00   0.00     0.01     0.01     0.02     0.02     0.02   562 1.00
lambda[29]      0.03    0.00   0.01     0.02     0.02     0.02     0.03     0.04   387 1.01
lambda[30]      0.03    0.00   0.01     0.02     0.02     0.02     0.03     0.04   387 1.01
lambda[31]      0.03    0.00   0.01     0.02     0.02     0.02     0.03     0.04   387 1.01
lambda[32]      0.03    0.00   0.01     0.02     0.02     0.02     0.03     0.04   387 1.01
lambda[33]      0.03    0.00   0.01     0.02     0.02     0.02     0.03     0.04   387 1.01
lambda[34]      0.02    0.00   0.00     0.01     0.01     0.02     0.02     0.02   562 1.00
lambda[35]      0.02    0.00   0.00     0.01     0.01     0.02     0.02     0.02   562 1.00
lambda[36]      0.02    0.00   0.00     0.01     0.01     0.02     0.02     0.02   562 1.00
lambda[37]      0.03    0.00   0.01     0.02     0.02     0.02     0.03     0.04   387 1.01
lambda[38]      0.02    0.00   0.00     0.01     0.01     0.02     0.02     0.02   562 1.00
lambda[39]      0.03    0.00   0.01     0.02     0.02     0.02     0.03     0.04   387 1.01
lambda[40]      0.02    0.00   0.00     0.01     0.01     0.02     0.02     0.02   562 1.00
lambda[41]      0.02    0.00   0.00     0.01     0.01     0.02     0.02     0.02   562 1.00
lambda[42]      0.02    0.00   0.00     0.01     0.01     0.02     0.02     0.02   562 1.00
lambda[43]      0.02    0.00   0.00     0.01     0.01     0.02     0.02     0.02   562 1.00
lambda[44]      0.03    0.00   0.01     0.02     0.02     0.02     0.03     0.04   387 1.01
lambda[45]      0.02    0.00   0.00     0.01     0.01     0.02     0.02     0.02   562 1.00
lambda[46]      0.03    0.00   0.01     0.02     0.02     0.02     0.03     0.04   387 1.01
lambda[47]      0.03    0.00   0.01     0.02     0.02     0.02     0.03     0.04   387 1.01
lambda[48]      0.03    0.00   0.01     0.02     0.02     0.02     0.03     0.04   387 1.01
lambda[49]      0.02    0.00   0.00     0.01     0.01     0.02     0.02     0.02   562 1.00
lambda[50]      0.02    0.00   0.00     0.01     0.01     0.02     0.02     0.02   562 1.00
lambda[51]      0.03    0.00   0.01     0.02     0.02     0.02     0.03     0.04   387 1.01
lambda[52]      0.02    0.00   0.00     0.01     0.01     0.02     0.02     0.02   562 1.00
lambda[53]      0.03    0.00   0.01     0.02     0.02     0.02     0.03     0.04   387 1.01
lambda[54]      0.03    0.00   0.01     0.02     0.02     0.02     0.03     0.04   387 1.01
lambda[55]      0.02    0.00   0.00     0.01     0.01     0.02     0.02     0.02   562 1.00
lambda[56]      0.02    0.00   0.00     0.01     0.01     0.02     0.02     0.02   562 1.00
lambda[57]      0.03    0.00   0.01     0.02     0.02     0.02     0.03     0.04   387 1.01
lambda[58]      0.03    0.00   0.01     0.02     0.02     0.02     0.03     0.04   387 1.01
lambda[59]      0.03    0.00   0.01     0.02     0.02     0.02     0.03     0.04   387 1.01
lambda[60]      0.03    0.00   0.01     0.02     0.02     0.02     0.03     0.04   387 1.01
lambda[61]      0.03    0.00   0.01     0.02     0.02     0.02     0.03     0.04   387 1.01
lambda[62]      0.03    0.00   0.01     0.02     0.02     0.02     0.03     0.04   387 1.01
lambda[63]      0.03    0.00   0.01     0.02     0.02     0.02     0.03     0.04   387 1.01
lambda[64]      0.02    0.00   0.00     0.01     0.01     0.02     0.02     0.02   562 1.00
lambda[65]      0.03    0.00   0.01     0.02     0.02     0.02     0.03     0.04   387 1.01
lambda[66]      0.02    0.00   0.00     0.01     0.01     0.02     0.02     0.02   562 1.00
lambda[67]      0.02    0.00   0.00     0.01     0.01     0.02     0.02     0.02   562 1.00
lambda[68]      0.02    0.00   0.00     0.01     0.01     0.02     0.02     0.02   562 1.00
lambda[69]      0.03    0.00   0.01     0.02     0.02     0.02     0.03     0.04   387 1.01
lambda[70]      0.03    0.00   0.01     0.02     0.02     0.02     0.03     0.04   387 1.01
lambda[71]      0.02    0.00   0.00     0.01     0.01     0.02     0.02     0.02   562 1.00
lambda[72]      0.03    0.00   0.01     0.02     0.02     0.02     0.03     0.04   387 1.01
lambda[73]      0.02    0.00   0.00     0.01     0.01     0.02     0.02     0.02   562 1.00
lambda[74]      0.03    0.00   0.01     0.02     0.02     0.02     0.03     0.04   387 1.01
lambda[75]      0.03    0.00   0.01     0.02     0.02     0.02     0.03     0.04   387 1.01
lambda[76]      0.02    0.00   0.00     0.01     0.01     0.02     0.02     0.02   562 1.00
lambda[77]      0.03    0.00   0.01     0.02     0.02     0.02     0.03     0.04   387 1.01
lambda[78]      0.03    0.00   0.01     0.02     0.02     0.02     0.03     0.04   387 1.01
lambda[79]      0.02    0.00   0.00     0.01     0.01     0.02     0.02     0.02   562 1.00
lambda[80]      0.03    0.00   0.01     0.02     0.02     0.02     0.03     0.04   387 1.01
lambda[81]      0.03    0.00   0.01     0.02     0.02     0.02     0.03     0.04   387 1.01
lambda[82]      0.03    0.00   0.01     0.02     0.02     0.02     0.03     0.04   387 1.01
lambda[83]      0.03    0.00   0.01     0.02     0.02     0.02     0.03     0.04   387 1.01
lambda[84]      0.03    0.00   0.01     0.02     0.02     0.02     0.03     0.04   387 1.01
lambda[85]      0.02    0.00   0.00     0.01     0.01     0.02     0.02     0.02   562 1.00
lambda[86]      0.03    0.00   0.01     0.02     0.02     0.02     0.03     0.04   387 1.01
lambda[87]      0.02    0.00   0.00     0.01     0.01     0.02     0.02     0.02   562 1.00
lambda[88]      0.03    0.00   0.01     0.02     0.02     0.02     0.03     0.04   387 1.01
lambda[89]      0.03    0.00   0.01     0.02     0.02     0.02     0.03     0.04   387 1.01
lambda[90]      0.02    0.00   0.00     0.01     0.01     0.02     0.02     0.02   562 1.00
lambda[91]      0.02    0.00   0.00     0.01     0.01     0.02     0.02     0.02   562 1.00
lambda[92]      0.02    0.00   0.00     0.01     0.01     0.02     0.02     0.02   562 1.00
lambda[93]      0.02    0.00   0.00     0.01     0.01     0.02     0.02     0.02   562 1.00
lambda[94]      0.03    0.00   0.01     0.02     0.02     0.02     0.03     0.04   387 1.01
lambda[95]      0.02    0.00   0.00     0.01     0.01     0.02     0.02     0.02   562 1.00
 [ reached getOption("max.print") -- omitted 306 rows ]

Samples were drawn using NUTS(diag_e) at Fri Nov 15 21:02:46 2024.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
